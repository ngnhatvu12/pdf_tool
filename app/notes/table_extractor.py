from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import io, re, os
import fitz  # PyMuPDF
import pdfplumber
import camelot
import pandas as pd
from PIL import Image
import pytesseract
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# --- ENV (Windows optional) ---
TESSERACT_PATH = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
if os.path.exists(TESSERACT_PATH):
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH

# --------- regex & flags ----------
NUM_ANY  = re.compile(r"\(?[-+]?\s*(?:\d{1,3}(?:[.\s]\d{3})+|\d+)(?:,\d+|\.\d+)?\)?$")
NUMISH   = re.compile(r'^[\d\.\,\(\)\-\s]+$')
DATE_RE  = re.compile(r'(\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4})')

# B·∫≠t / t·∫Øt log OCR raw lines (n·∫øu nhi·ªÅu log qu√° anh c√≥ th·ªÉ ƒë·∫∑t = False)
DEBUG_OCR_LINES = True


# ---------- OCR basic ----------
def _ocr_image(pdf_path: Path, pageno1: int, dpi=420) -> Image.Image:
    """
    Render 1 trang PDF -> ·∫£nh RGB ƒë·ªÉ d√πng cho OCR.
    """
    with fitz.open(pdf_path) as doc:
        p = doc.load_page(pageno1 - 1)
        pm = p.get_pixmap(dpi=dpi, colorspace=fitz.csRGB, alpha=False)
        return Image.open(io.BytesIO(pm.tobytes("png")))


def _ocr_words(img: Image.Image) -> pd.DataFrame:
    """
    Tr·∫£ df(word,x,y,w,h,conf) t·ª´ Tesseract.
    - Pass 1: psm6 (mode block) v·ªõi ng∆∞·ª°ng conf>=8
    - N·∫øu token qu√° √≠t -> Pass 2: psm4 (mode paragraph), kh√¥ng ch·∫∑n conf.
    B·ªô l·ªçc r·∫•t nh·∫π tay ƒë·ªÉ kh√¥ng b·ªè s√≥t text.
    """
    cfg = r'--oem 3 --psm 6 -l vie+eng -c preserve_interword_spaces=1'
    d = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, config=cfg)
    rows = []
    for i, txt in enumerate(d["text"]):
        t = (txt or "").strip()
        if not t:
            continue
        conf = d["conf"][i]
        try:
            cf = float(conf)
        except Exception:
            cf = -1.0
        if cf < 8:
            continue
        rows.append({
            "text": t,
            "x": d["left"][i],
            "y": d["top"][i],
            "w": d["width"][i],
            "h": d["height"][i],
            "conf": cf
        })
    df = pd.DataFrame(rows)

    # Pass 2 n·∫øu token qu√° √≠t => c·ªë g·∫Øng OCR l·∫°i to√†n trang
    if len(df) < 80:
        cfg2 = r'--oem 3 --psm 4 -l vie+eng -c preserve_interword_spaces=1'
        d2 = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, config=cfg2)
        rows2 = []
        for i, txt in enumerate(d2["text"]):
            t = (txt or "").strip()
            if not t:
                continue
            conf = d2["conf"][i]
            try:
                cf = float(conf)
            except Exception:
                cf = -1.0
            rows2.append({
                "text": t,
                "x": d2["left"][i],
                "y": d2["top"][i],
                "w": d2["width"][i],
                "h": d2["height"][i],
                "conf": cf
            })
        df = pd.DataFrame(rows2)

    return df


def _group_lines(df: pd.DataFrame, y_tol=8) -> List[pd.DataFrame]:
    """
    Gom word theo d√≤ng logic d·ª±a v√†o to·∫° ƒë·ªô y.
    """
    if df.empty:
        return []
    df = df.sort_values(["y", "x"]).reset_index(drop=True)
    lines, cur = [], [df.iloc[0]]
    for i in range(1, len(df)):
        prev = cur[-1]
        row  = df.iloc[i]
        if abs(row["y"] - prev["y"]) <= y_tol:
            cur.append(row)
        else:
            lines.append(pd.DataFrame(cur))
            cur = [row]
    if cur:
        lines.append(pd.DataFrame(cur))
    return lines


def _merge_numeric_runs(texts, xs, gap_px=110):
    """
    Gh√©p c√°c token s·ªë ƒë·ª©ng c·∫°nh nhau th√†nh 1 s·ªë ƒë·∫ßy ƒë·ªß.
    B·ªè l·ªçc m·∫°nh, ch·ªâ lo·∫°i token kh√¥ng ph·∫£i NUMISH.
    """
    items = sorted([(x, t) for t, x in zip(texts, xs)], key=lambda z: z[0])
    out, buf, bx, prev_x = [], "", None, None

    def flush():
        nonlocal buf, bx
        if not buf:
            return
        raw = re.sub(r'[^0-9\.\,\(\)\-\s]', '', buf)
        if re.search(r'\d', raw):
            out.append((bx, raw.strip()))
        buf, bx = "", None

    for x, t in items:
        t = (t or "").strip()
        if not t:
            continue
        if not NUMISH.match(t):
            flush()
            prev_x = x
            continue

        if buf == "":
            buf, bx = t, x
        else:
            near = prev_x is not None and (x - prev_x) <= gap_px
            thousand_glue = buf.rstrip().endswith((".", ",")) or re.search(r"[\.\,]\s*$", buf)
            only_2_3 = len(re.sub(r"\D", "", t)) in (2, 3)
            if near or (thousand_glue and only_2_3):
                buf += t
            else:
                flush()
                buf, bx = t, x
        prev_x = x

    flush()
    return out


def _debug_dump_ocr_lines(words: pd.DataFrame, pageno1: int):
    """
    Log to√†n b·ªô d√≤ng OCR ƒë·ªçc ƒë∆∞·ª£c (tr∆∞·ªõc khi c·∫Øt header/footer), ƒë·ªÉ anh check
    OCR & b·ªô l·ªçc.
    """
    if not DEBUG_OCR_LINES or words.empty:
        return
    print(f"\n===== OCR RAW LINES ‚Äì page {pageno1} =====")
    for ln in _group_lines(words, y_tol=8):
        y0 = int(ln["y"].min())
        text = " ".join(str(t) for t in ln["text"].tolist())
        print(f"[p{pageno1:03d} y~{y0:04d}] {text}")


# ---------- pdfplumber / camelot ----------
def _extract_plumber_tables(pdf_path: Path, pageno1: int) -> List[pd.DataFrame]:
    """
    C·ªë g·∫Øng l·∫•y t·ªëi ƒëa b·∫£ng vector t·ª´ pdfplumber.
    B·ªô l·ªçc nh·∫π: ch·ªâ b·ªè b·∫£ng 1 c·ªôt ho·∫∑c to√†n r·ªóng.
    """
    out: List[pd.DataFrame] = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            page = pdf.pages[pageno1 - 1]

            # 1) find_tables (khung m·∫£nh)
            for tb in (page.find_tables() or []):
                df = pd.DataFrame(tb.extract()).fillna("").replace("\n", " ", regex=True)
                if df.shape[1] >= 2:
                    df = df.loc[:, ~(df.astype(str).apply(lambda s: (s.str.strip() == "").all()))]
                    if df.shape[1] >= 2:
                        out.append(df)

            # 2) lines-based
            tables = page.extract_tables(table_settings={
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "intersection_tolerance": 6,
                "snap_tolerance": 3,
                "join_tolerance": 3,
            }) or []

            # 3) text-based (kh√¥ng c√≥ khung)
            tables += page.extract_tables(table_settings={
                "vertical_strategy": "text",
                "horizontal_strategy": "text",
                "text_y_tolerance": 3,
                "text_x_tolerance": 2,
                "intersection_tolerance": 3,
                "snap_tolerance": 3,
                "join_tolerance": 3,
            }) or []

            for tb in tables:
                df = pd.DataFrame(tb).fillna("").replace("\n", " ", regex=True)
                if df.shape[1] >= 2:
                    df = df.loc[:, ~(df.astype(str).apply(lambda s: (s.str.strip() == "").all()))]
                    if df.shape[1] >= 2:
                        out.append(df)
    except Exception:
        pass
    return out


def _extract_camelot_tables(pdf_path: Path, pageno1: int) -> List[pd.DataFrame]:
    """
    Camelot stream-mode cho b·∫£ng text nhi·ªÅu ƒë∆∞·ªùng k·∫ª m·∫£nh.
    Kh√¥ng filter g√¨ ngo√†i s·ªë c·ªôt.
    """
    out: List[pd.DataFrame] = []
    try:
        tables = camelot.read_pdf(str(pdf_path), pages=str(pageno1), flavor="stream")
        for t in tables:
            df = t.df.replace("\n", " ", regex=True)
            if df.shape[1] >= 2:
                out.append(df)
    except Exception:
        pass
    return out


# ---------- multi-column OCR (header + numeric clusters) ----------
def _header_column_centers(words_df: pd.DataFrame) -> List[int]:
    """
    ∆Ø·ªõc l∆∞·ª£ng v·ªã tr√≠ c·ªôt t·ª´ v√πng header (text).
    Ch·ªâ d√πng ƒë·ªÉ h·ªó tr·ª£, kh√¥ng quy·∫øt ƒë·ªãnh ch√≠nh.
    """
    if words_df.empty:
        return []
    y_min, y_max = words_df["y"].min(), words_df["y"].max()
    header = words_df[words_df["y"] <= (y_min + 0.30 * (y_max - y_min))].copy()
    if header.empty:
        return []
    xs = []
    for _, r in header.iterrows():
        t = (r["text"] or "").strip()
        tl = t.lower()
        if NUM_ANY.fullmatch(t):
            continue
        if len(t) < 3:
            continue
        if any(k in tl for k in ["ƒë∆°n v·ªã", "don vi", "vnd", "c·ªôt", "cot", "m·ª•c", "muc"]):
            continue
        xs.append(int(r["x"]))
    xs.sort()
    centers = []
    for x in xs:
        if not centers or abs(x - centers[-1]) > 55:
            centers.append(x)
        else:
            centers[-1] = int((centers[-1] + x) / 2)
    return centers


def _max_big_numbers_per_line(words_df: pd.DataFrame) -> int:
    """
    ƒê·∫øm s·ªë l∆∞·ª£ng gi√° tr·ªã s·ªë "l·ªõn" (>=5 ch·ªØ s·ªë) t·ªëi ƒëa tr√™n 1 d√≤ng.
    D√πng ƒë·ªÉ ph√¢n bi·ªát trang text vs trang b·∫£ng.
    """
    if words_df.empty:
        return 0
    lines = _group_lines(words_df, y_tol=8)
    mx = 0
    for ln in lines:
        texts = ln["text"].tolist()
        xs    = ln["x"].tolist()
        merged = _merge_numeric_runs(texts, xs, gap_px=110)
        cnt = 0
        for _x, raw in merged:
            if len(re.sub(r"[^\d]", "", str(raw))) >= 5:
                cnt += 1
        mx = max(mx, cnt)
    return mx


def _extract_header_text_near(words_df: pd.DataFrame, x: int, tol: int = 200) -> str:
    if words_df.empty:
        return ""
    y_min, y_max = words_df["y"].min(), words_df["y"].max()
    y_cut = y_min + 0.20 * (y_max - y_min)
    header = words_df[words_df["y"] <= y_cut]
    col_words = header[(header["x"] >= x - tol) & (header["x"] <= x + tol)]
    return " ".join(col_words["text"].tolist()).strip().lower()


def _normalize_context_from_header(txt: str) -> Tuple[str, str]:
    """
    Chuy·ªÉn header text -> context_key + context_label.
    B·ªô nh·∫≠n di·ªán ƒë∆°n gi·∫£n, ƒë·ªß d√πng cho notes.
    """
    t = (txt or "").lower()

    m = re.search(r'qu[√Ωy]\s*(\d+)\s*[\/\-]?\s*(20\d{2})', t)
    if m:
        return (f"q{m.group(1)}_{m.group(2)}", f"Qu√Ω {m.group(1)}/{m.group(2)}")

    if any(k in t for k in ["k·ª≥ k·∫ø to√°n", "ky ke toan", "l≈©y k·∫ø", "luy ke", "6 th√°ng", "6 thang"]):
        y = re.search(r'(20\d{2})', t)
        return (f"ytd_{y.group(1) if y else 'unk'}", "YTD")

    dm = DATE_RE.findall(t)
    if dm:
        y = re.search(r'(20\d{2})', dm[-1])
        return (f"asof_{y.group(1) if y else 'unk'}", f"As of {dm[-1]}")

    if "ƒë·∫ßu k·ª≥" in t or "opening" in t:
        return ("opening", "ƒê·∫ßu k·ª≥")
    if "cu·ªëi k·ª≥" in t or "closing" in t:
        return ("closing", "Cu·ªëi k·ª≥")

    # fallback: gi·ªØ nguy√™n text ƒë·ªÉ parser suy lu·∫≠n ti·∫øp
    return ("col", txt or "")


def _cluster_numeric_columns(words_df: pd.DataFrame, max_k: int = 8) -> List[int]:
    """
    Gom c·ª•m c√°c to·∫° ƒë·ªô x c·ªßa s·ªë li·ªáu ‚Üí suy ra v·ªã tr√≠ c·ªôt.

    S·ª≠a:
      - D√πng _max_big_numbers_per_line ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng s·ªë c·ªôt t·ªëi thi·ªÉu (min_k),
        tr√°nh case k qu√° nh·ªè (v√≠ d·ª•: ch·ªçn 2 c·ªôt trong khi 1 d√≤ng c√≥ 6‚Äì7 s·ªë).
      - V·∫´n l·ªçc c·ªôt y·∫øu: m·ªói center ph·∫£i c√≥ ƒë·ªß s·ªë ƒëi·ªÉm (>=3) m·ªõi ch·∫•p nh·∫≠n.
    """
    if words_df.empty:
        return []

    # gom to√†n b·ªô s·ªë l·ªõn theo line
    xs_points: List[int] = []
    for ln in _group_lines(words_df, y_tol=8):
        texts = ln["text"].tolist()
        xs    = ln["x"].tolist()
        merged = _merge_numeric_runs(texts, xs, gap_px=110)
        for x, raw in merged:
            digits = re.sub(r"[^\d]", "", str(raw))
            if len(digits) >= 5:  # ch·ªâ coi l√† s·ªë li·ªáu ti·ªÅn / s·ªë l∆∞·ª£ng l·ªõn
                xs_points.append(int(x))

    if not xs_points:
        return []

    X = np.array(xs_points).reshape(-1, 1)

    # ∆∞·ªõc l∆∞·ª£ng s·ªë c·ªôt t·ªëi thi·ªÉu t·ª´ d√≤ng "gi√†u" s·ªë nh·∫•t
    max_big = _max_big_numbers_per_line(words_df)
    # √≠t nh·∫•t 2 c·ªôt, nhi·ªÅu nh·∫•t max_k
    start_k = max(2, min(max_big, max_k))
    end_k   = min(max_k, len(X))

    best_centers, best_score = None, -1.0
    for k in range(start_k, end_k + 1):
        try:
            km = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X)
            if len(set(km.labels_)) < 2:
                continue
            sc = silhouette_score(X, km.labels_)
            # ∆∞u ti√™n k l·ªõn h∆°n n·∫øu score kh√¥ng t·ªá h∆°n qu√° 0.02
            prefer = (best_centers is not None and k > len(best_centers) and sc >= best_score - 0.02)
            if sc > best_score or prefer:
                best_score = sc
                best_centers = [int(c[0]) for c in km.cluster_centers_]
        except Exception:
            continue

    centers_num = sorted(best_centers) if best_centers else []

    # L·ªçc c√°c center y·∫øu: m·ªói center ph·∫£i c√≥ >=3 ƒëi·ªÉm s·ªë l·ªõn
    strong_centers: List[int] = []
    for cx in centers_num:
        cnt = sum(1 for x in xs_points if abs(x - cx) <= 40)
        if cnt >= 3:
            strong_centers.append(int(cx))

    centers_hdr = _header_column_centers(words_df)

    # G·ªôp 2 ngu·ªìn (numeric + header)
    all_c = sorted(strong_centers + centers_hdr)
    merged: List[int] = []
    for x in all_c:
        if not merged or abs(x - merged[-1]) > 55:
            merged.append(int(x))
        else:
            merged[-1] = int((merged[-1] + x) // 2)

    if len(merged) < 2:
        return merged
    return merged[:max_k]

def _infer_multi_columns(words_df: pd.DataFrame) -> List[Dict]:
    """
    Tr·∫£ list [{x, context_key, context_label}] t·ª´ tr√°i sang ph·∫£i.
    Kh√¥ng c√≥ c·ªôt n√†o ‚Üí tr·∫£ [].
    """
    centers = _cluster_numeric_columns(words_df, max_k=8)
    if not centers or len(centers) < 2:
        return []

    cols: List[Dict] = []
    used = set()
    for idx, x in enumerate(sorted(centers), 1):
        h = _extract_header_text_near(words_df, x)
        key, lbl = _normalize_context_from_header(h)
        if key == "col":
            key = f"col{idx}"
        if key in used:
            key = f"{key}_{idx}"
        used.add(key)
        cols.append({"x": int(x), "context_key": key, "context_label": lbl})
    return cols


def _parse_line_with_multi_columns(line_df: pd.DataFrame, cols: List[Dict], tol=260):
    """
    T·ª´ 1 d√≤ng OCR + danh s√°ch c·ªôt (to·∫° ƒë·ªô x) ‚Üí label + dict context_key -> raw_number.

    S·ª≠a:
      - V·∫´n d√πng v√πng b√™n ph·∫£i ƒë·ªÉ l·∫•y s·ªë li·ªáu (tr√°nh s·ªë n·∫±m trong STT, nƒÉm...).
      - Label ƒë∆∞·ª£c gh√©p t·ª´ T·∫§T C·∫¢ token kh√¥ng ph·∫£i s·ªë tr√™n d√≤ng (tr·ª´ VND/VNƒê),
        kh√¥ng gi·ªõi h·∫°n ch·ªâ ·ªü v√πng b√™n tr√°i ‚Üí x·ª≠ l√Ω t·ªët case m√¥ t·∫£ n·∫±m b√™n ph·∫£i
        nh∆∞ "Gi√° tr·ªã gia tƒÉng", "Thu nh·∫≠p doanh nghi·ªáp", ...
    """
    if line_df.empty or not cols:
        return "", {}

    texts = line_df["text"].tolist()
    xs    = line_df["x"].tolist()
    ws    = line_df["w"].tolist() if "w" in line_df.columns else [20] * len(xs)

    left_most_col = min(c["x"] for c in cols)
    median_w = int(np.median([w for w in ws if w and w > 0])) if ws else 24
    margin   = max(30, int(1.5 * median_w))

    # V√πng label ƒë·ªÉ LO·∫†I S·ªê (s·ªë ·ªü qu√° b√™n tr√°i th∆∞·ªùng l√† STT, nƒÉm...)
    label_threshold_x = left_most_col - margin

    # --- 1) Gom s·ªë li·ªáu ---
    merged = _merge_numeric_runs(texts, xs, gap_px=110)
    nums: List[Tuple[int, str]] = []
    for x, raw in merged:
        digits = re.sub(r"[^\d]", "", raw)
        # B·ªè to√†n b·ªô s·ªë n·∫±m trong v√πng label (th∆∞·ªùng l√† s·ªë m·ª•c, s·ªë nƒÉm)
        if x < label_threshold_x:
            continue
        # Nh·∫≠n t·∫•t c·∫£ s·ªë c√≥ >=3 ch·ªØ s·ªë (gi·ªØ EPS / s·ªë l∆∞·ª£ng nh·ªè trong b·∫£ng)
        if len(digits) >= 3:
            nums.append((int(x), raw))

    if not nums:
        return "", {}

    # --- 2) Gh√©p label t·ª´ t·∫•t c·∫£ token kh√¥ng ph·∫£i s·ªë ---
    label_tokens: List[str] = []
    for t in texts:
        t_clean = (t or "").strip()
        if not t_clean:
            continue
        tl = t_clean.lower()
        if tl in {"vnd", "vnƒë"}:
            # b·ªè ƒë∆°n v·ªã ti·ªÅn ·ªü cu·ªëi d√≤ng
            continue
        if NUMISH.match(t_clean):
            # gi·ªëng s·ªë ‚Üí kh√¥ng ƒë∆∞a v√†o label
            continue
        label_tokens.append(t_clean)

    label = " ".join(label_tokens).strip()

    cols_sorted = sorted([(c["x"], c["context_key"]) for c in cols], key=lambda z: z[0])
    nums_sorted = sorted([(x, s) for x, s in nums], key=lambda z: z[0])

    def assign_once(limit: int):
        assigned, used_num = {}, set()
        pairs = []
        for ci, (cx, ck) in enumerate(cols_sorted):
            for ni, (nx, sv) in enumerate(nums_sorted):
                d = abs(nx - cx)
                if d <= limit:
                    pairs.append((d, ci, ni))
        for d, ci, ni in sorted(pairs, key=lambda z: z[0]):
            if ci in assigned or ni in used_num:
                continue
            assigned[ci] = ni
            used_num.add(ni)
        return assigned

    assigned = assign_once(tol)
    if len(assigned) < len(cols_sorted):
        assigned = assign_once(int(2.2 * tol))

    values: Dict[str, str] = {}

    # fallback: n·∫øu kh√¥ng match to·∫° ƒë·ªô nh∆∞ng s·ªë l∆∞·ª£ng g·∫ßn b·∫±ng s·ªë c·ªôt ‚áí g√°n tu·∫ßn t·ª±
    if len(assigned) == 0 and 1 <= len(nums_sorted) <= len(cols_sorted) + 1:
        for (cx, ck), (_nx, sv) in zip(cols_sorted, nums_sorted):
            values[ck] = sv
    else:
        for ci, (cx, ck) in enumerate(cols_sorted):
            if ci in assigned:
                ni = assigned[ci]
                values[ck] = nums_sorted[ni][1]

    return label, values


def _page_is_tabular(words: pd.DataFrame) -> bool:
    """
    Heuristic m·∫°nh ƒë·ªÉ ph√¢n bi·ªát trang thuy·∫øt minh d·∫°ng text vs trang c√≥ b·∫£ng s·ªë li·ªáu:
      - Ph·∫£i c√≥ √≠t nh·∫•t 2 c·ªôt s·ªë l·ªõn (>=5 ch·ªØ s·ªë)
      - M·ªói c·ªôt c√≥ >=3 ƒëi·ªÉm s·ªë.
    """
    if words.empty:
        return False
    if _max_big_numbers_per_line(words) < 2:
        # m·ªói d√≤ng t·ªëi ƒëa <2 s·ªë l·ªõn ‚Üí nhi·ªÅu kh·∫£ nƒÉng ch·ªâ l√† text, kh√¥ng ph·∫£i b·∫£ng
        return False
    centers = _cluster_numeric_columns(words, max_k=8)
    return len(centers) >= 2


def _extract_ocr_table(pdf_path: Path, pageno1: int) -> Optional[pd.DataFrame]:
    """
    D√≤ b·∫£ng b·∫±ng OCR cho 1 trang:
      - Log to√†n b·ªô d√≤ng OCR ƒë·ªçc ƒë∆∞·ª£c (DEBUG_OCR_LINES)
      - C·∫Øt header/footer
      - Suy ƒëo√°n nhi·ªÅu c·ªôt s·ªë t·ª´ s·ªë l·ªõn (>=5 ch·ªØ s·ªë)
      - Parse t·ª´ng d√≤ng -> DataFrame(desc, col_1_label, col_2_label,...)

    C√°c trang thuy·∫øt minh thu·∫ßn text (ch√≠nh s√°ch k·∫ø to√°n...) s·∫Ω b·ªã lo·∫°i
    v√¨ kh√¥ng ƒë·∫°t ti√™u ch√≠ ‚Äútrang b·∫£ng‚Äù.
    """
    img = _ocr_image(pdf_path, pageno1)
    words = _ocr_words(img)
    if words.empty:
        return None

    # Log raw OCR lines tr∆∞·ªõc m·ªçi filter
    _debug_dump_ocr_lines(words, pageno1)

    # C·∫Øt ph·∫ßn gi·ªØa trang (b·ªè header/footer m·∫°nh ch·ªØ "nƒÉm yyyy")
    y_min, y_max = words["y"].min(), words["y"].max()
    height = max(1, y_max - y_min)
    work = words[(words["y"] >= y_min + 0.08 * height) &
                 (words["y"] <= y_max - 0.12 * height)].copy()
    if work.empty:
        work = words

    # N·∫øu trang kh√¥ng c√≥ c·∫•u tr√∫c b·∫£ng r√µ r√†ng ‚Üí b·ªè
    if not _page_is_tabular(work):
        print(f"üîé [OCR] page {pageno1}: looks like narrative text, skip as table.")
        return None

    cols = _infer_multi_columns(work)

    # Fallback: n·∫øu v·∫´n kh√¥ng suy ra ƒë∆∞·ª£c multi-column th√¨ th√¥i, kh√¥ng coi l√† b·∫£ng OCR
    if not cols:
        print(f"‚ö†Ô∏è [OCR] page {pageno1}: cannot infer numeric columns, skip.")
        return None

    # T√™n c·ªôt hi·ªÉn th·ªã (header) ‚Äì d√πng context_label n·∫øu c√≥, ng∆∞·ª£c l·∫°i fallback "col1.."
    header_names: List[str] = ["desc"]
    for idx, c in enumerate(cols, 1):
        label = c["context_label"] or c["context_key"] or f"col{idx}"
        label = label.strip() or f"col{idx}"
        header_names.append(label)

    lines = _group_lines(work, y_tol=8)

    recs: List[dict] = []
    last_label = ""
    for ln in lines:
        label, vals = _parse_line_with_multi_columns(ln, cols)
        if not label:
            label = last_label
        if not vals:
            # d√≤ng kh√¥ng c√≥ s·ªë ‚Üí c√≥ th·ªÉ l√† heading, b·ªè qua ·ªü ƒë√¢y
            continue
        if not label:
            label = "(no_label)"
        last_label = label

        row: Dict[str, Optional[str]] = {header_names[0]: label}
        for j, c in enumerate(cols, 1):
            ctx_key = c["context_key"]
            col_name = header_names[j]
            row[col_name] = vals.get(ctx_key)
        recs.append(row)

    if not recs:
        return None

    df = pd.DataFrame(recs)
    # ƒë·∫£m b·∫£o th·ª© t·ª± c·ªôt ·ªïn ƒë·ªãnh: desc, col1, col2, ...
    df = df.reindex(columns=header_names)
    df = df.drop_duplicates()
    return df


def harvest_tables(pdf_path: Path, a: int, b: int) -> List[Tuple[pd.DataFrame, int, str]]:
    """
    Qu√©t [a,b] ‚Üí [(df, page, mode)] v·ªõi mode ‚àà {"vector","camelot","ocr"}.
    - Lu√¥n c·ªë g·∫Øng l·∫•y *h·∫øt* b·∫£ng (k·ªÉ c·∫£ b·∫£ng kh√¥ng khung).
    - OCR lu√¥n ch·∫°y b·ªï sung, kh√¥ng ph·ª• thu·ªôc ƒë√£ c√≥ vector hay ch∆∞a.
    - C√°c trang thuy·∫øt minh text thu·∫ßn t√∫y s·∫Ω b·ªã _extract_ocr_table b·ªè qua.
    """
    out: List[Tuple[pd.DataFrame, int, str]] = []
    for p in range(a, b + 1):
        # pdfplumber vector
        for df in _extract_plumber_tables(pdf_path, p):
            out.append((df, p, "vector"))
        # camelot stream
        for df in _extract_camelot_tables(pdf_path, p):
            out.append((df, p, "camelot"))
        # OCR multi-column
        mdf = _extract_ocr_table(pdf_path, p)
        if mdf is not None and not mdf.empty:
            out.append((mdf, p, "ocr"))
    return out
