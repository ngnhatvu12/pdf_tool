from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import io, re, os
import fitz  # PyMuPDF
import pdfplumber
import camelot
import pandas as pd
from PIL import Image
import pytesseract
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

# --- ENV (Windows optional) ---
TESSERACT_PATH = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
if os.path.exists(TESSERACT_PATH):
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH

# --------- regex & flags ----------
NUM_ANY  = re.compile(r"\(?[-+]?\s*(?:\d{1,3}(?:[.\s]\d{3})+|\d+)(?:,\d+|\.\d+)?\)?$")
NUMISH   = re.compile(r'^[\d\.\,\(\)\-\s]+$')
DATE_RE  = re.compile(r'(\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4})')
CLEAN_NUM_CHARS_RE = re.compile(r"[^\d\-\+,\.]")
# B·∫≠t / t·∫Øt log OCR raw lines (n·∫øu nhi·ªÅu log qu√° anh c√≥ th·ªÉ ƒë·∫∑t = False)
DEBUG_OCR_LINES = False


# ---------- OCR basic ----------
def _ocr_image(pdf_path: Path, pageno1: int, dpi=320) -> Image.Image:
    """
    Render 1 trang PDF -> ·∫£nh RGB ƒë·ªÉ d√πng cho OCR.
    DPI=320: ƒë·ªß n√©t cho b·∫£ng s·ªë, nh·∫π h∆°n nhi·ªÅu so v·ªõi 420.
    """
    with fitz.open(pdf_path) as doc:
        p = doc.load_page(pageno1 - 1)
        pm = p.get_pixmap(dpi=dpi, colorspace=fitz.csRGB, alpha=False)
        return Image.open(io.BytesIO(pm.tobytes("png")))


def _ocr_words(img: Image.Image) -> pd.DataFrame:
    """
    Tr·∫£ df(word,x,y,w,h,conf) t·ª´ Tesseract.
    - Pass 1: psm6 (mode block) v·ªõi ng∆∞·ª°ng conf>=8
    - N·∫øu token qu√° √≠t -> Pass 2: psm4 (mode paragraph), kh√¥ng ch·∫∑n conf.
    B·ªô l·ªçc r·∫•t nh·∫π tay ƒë·ªÉ kh√¥ng b·ªè s√≥t text.
    """
    cfg = r'--oem 3 --psm 6 -l vie+eng -c preserve_interword_spaces=1'
    d = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, config=cfg)
    rows = []
    for i, txt in enumerate(d["text"]):
        t = (txt or "").strip()
        if not t:
            continue
        conf = d["conf"][i]
        try:
            cf = float(conf)
        except Exception:
            cf = -1.0
        if cf < 8:
            continue
        rows.append({
            "text": t,
            "x": d["left"][i],
            "y": d["top"][i],
            "w": d["width"][i],
            "h": d["height"][i],
            "conf": cf
        })
    df = pd.DataFrame(rows)

    # Pass 2 n·∫øu token qu√° √≠t => c·ªë g·∫Øng OCR l·∫°i to√†n trang
    if len(df) < 80:
        cfg2 = r'--oem 3 --psm 4 -l vie+eng -c preserve_interword_spaces=1'
        d2 = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, config=cfg2)
        rows2 = []
        for i, txt in enumerate(d2["text"]):
            t = (txt or "").strip()
            if not t:
                continue
            conf = d2["conf"][i]
            try:
                cf = float(conf)
            except Exception:
                cf = -1.0
            rows2.append({
                "text": t,
                "x": d2["left"][i],
                "y": d2["top"][i],
                "w": d2["width"][i],
                "h": d2["height"][i],
                "conf": cf
            })
        df = pd.DataFrame(rows2)

    return df

def _clean_numeric_str(raw: str) -> str:
    if raw is None:
        return ""
    s = str(raw).strip()
    if not s:
        return ""
    if s.startswith("(") and s.endswith(")"):
        s = s[1:-1]
    s = CLEAN_NUM_CHARS_RE.sub("", s)
    return s

# Re-use b·ªô fix label gi·ªëng b√™n ocr_processor
LABEL_FIX_PATTERNS = {
    "ti·ªÅn kh√¥ng k·ª≥ h·∫°n h√†ng g·ª≠i ng√¢n": "Ti·ªÅn g·ª≠i ng√¢n h√†ng kh√¥ng k·ª≥ h·∫°n",
    "chuy√™n ti·ªÅn ƒëang": "Ti·ªÅn ƒëang chuy·ªÉn",
    "ti·ªÅn ƒë∆∞∆°ng kho·∫£n c√°c t∆∞∆°ng": "C√°c kho·∫£n t∆∞∆°ng ƒë∆∞∆°ng ti·ªÅn",
}

def _normalize_label_text(label: str) -> str:
    if not label:
        return label
    low = re.sub(r"\s+", " ", label.lower()).strip().strip(":")
    for bad, good in LABEL_FIX_PATTERNS.items():
        if bad in low:
            return good
    return label.strip()

def _group_lines(df: pd.DataFrame, y_tol=8) -> List[pd.DataFrame]:
    """
    Gom word theo d√≤ng logic d·ª±a v√†o to·∫° ƒë·ªô y.
    """
    if df.empty:
        return []
    df = df.sort_values(["y", "x"]).reset_index(drop=True)
    lines, cur = [], [df.iloc[0]]
    for i in range(1, len(df)):
        prev = cur[-1]
        row  = df.iloc[i]
        if abs(row["y"] - prev["y"]) <= y_tol:
            cur.append(row)
        else:
            lines.append(pd.DataFrame(cur))
            cur = [row]
    if cur:
        lines.append(pd.DataFrame(cur))
    return lines


def _merge_numeric_runs(texts, xs, gap_px=110):
    """
    Gh√©p c√°c token s·ªë ƒë·ª©ng c·∫°nh nhau th√†nh 1 s·ªë ƒë·∫ßy ƒë·ªß.
    B·ªè ngo·∫∑c bao quanh, b·ªè k√Ω t·ª± l·∫° ‚Äì gi·ªØ 0-9 . , + -.
    """
    items = sorted([(x, t) for t, x in zip(texts, xs)], key=lambda z: z[0])
    out, buf, bx, prev_x = [], "", None, None

    def flush():
        nonlocal buf, bx
        if not buf:
            return
        cleaned = _clean_numeric_str(buf)
        if not cleaned or not re.search(r"\d", cleaned):
            buf, bx = "", None
            return
        out.append((bx, cleaned.strip()))
        buf, bx = "", None

    for x, t in items:
        t = (t or "").strip()
        if not t:
            continue
        if not NUMISH.match(t):
            flush()
            prev_x = x
            continue

        if buf == "":
            buf, bx = t, x
        else:
            near = prev_x is not None and (x - prev_x) <= gap_px
            thousand_glue = buf.rstrip().endswith((".", ",")) or re.search(r"[\.\,]\s*$", buf)
            only_2_3 = len(re.sub(r"\D", "", t)) in (2, 3)
            if near or (thousand_glue and only_2_3):
                buf += t
            else:
                flush()
                buf, bx = t, x
        prev_x = x

    flush()
    return out



def _debug_dump_ocr_lines(words: pd.DataFrame, pageno1: int):
    """
    Log to√†n b·ªô d√≤ng OCR ƒë·ªçc ƒë∆∞·ª£c (tr∆∞·ªõc khi c·∫Øt header/footer), ƒë·ªÉ anh check
    OCR & b·ªô l·ªçc.
    """
    if not DEBUG_OCR_LINES or words.empty:
        return
    print(f"\n===== OCR RAW LINES ‚Äì page {pageno1} =====")
    for ln in _group_lines(words, y_tol=8):
        y0 = int(ln["y"].min())
        text = " ".join(str(t) for t in ln["text"].tolist())
        print(f"[p{pageno1:03d} y~{y0:04d}] {text}")


# ---------- pdfplumber / camelot ----------
def _extract_plumber_tables(pdf_path: Path, pageno1: int) -> List[pd.DataFrame]:
    """
    C·ªë g·∫Øng l·∫•y t·ªëi ƒëa b·∫£ng vector t·ª´ pdfplumber.
    B·ªô l·ªçc nh·∫π: ch·ªâ b·ªè b·∫£ng 1 c·ªôt ho·∫∑c to√†n r·ªóng.
    """
    out: List[pd.DataFrame] = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            page = pdf.pages[pageno1 - 1]

            # 1) find_tables (khung m·∫£nh)
            for tb in (page.find_tables() or []):
                df = pd.DataFrame(tb.extract()).fillna("").replace("\n", " ", regex=True)
                if df.shape[1] >= 2:
                    df = df.loc[:, ~(df.astype(str).apply(lambda s: (s.str.strip() == "").all()))]
                    if df.shape[1] >= 2:
                        out.append(df)

            # 2) lines-based
            tables = page.extract_tables(table_settings={
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "intersection_tolerance": 6,
                "snap_tolerance": 3,
                "join_tolerance": 3,
            }) or []

            # 3) text-based (kh√¥ng c√≥ khung)
            tables += page.extract_tables(table_settings={
                "vertical_strategy": "text",
                "horizontal_strategy": "text",
                "text_y_tolerance": 3,
                "text_x_tolerance": 2,
                "intersection_tolerance": 3,
                "snap_tolerance": 3,
                "join_tolerance": 3,
            }) or []

            for tb in tables:
                df = pd.DataFrame(tb).fillna("").replace("\n", " ", regex=True)
                if df.shape[1] >= 2:
                    df = df.loc[:, ~(df.astype(str).apply(lambda s: (s.str.strip() == "").all()))]
                    if df.shape[1] >= 2:
                        out.append(df)
    except Exception:
        pass
    return out


def _extract_camelot_tables(pdf_path: Path, pageno1: int) -> List[pd.DataFrame]:
    """
    Camelot stream-mode cho b·∫£ng text nhi·ªÅu ƒë∆∞·ªùng k·∫ª m·∫£nh.
    Kh√¥ng filter g√¨ ngo√†i s·ªë c·ªôt.
    """
    out: List[pd.DataFrame] = []
    try:
        tables = camelot.read_pdf(str(pdf_path), pages=str(pageno1), flavor="stream")
        for t in tables:
            df = t.df.replace("\n", " ", regex=True)
            if df.shape[1] >= 2:
                out.append(df)
    except Exception:
        pass
    return out


# ---------- multi-column OCR (header + numeric clusters) ----------
def _header_column_centers(words_df: pd.DataFrame) -> List[int]:
    """
    ∆Ø·ªõc l∆∞·ª£ng v·ªã tr√≠ c·ªôt t·ª´ v√πng header (text).
    Ch·ªâ d√πng ƒë·ªÉ h·ªó tr·ª£, kh√¥ng quy·∫øt ƒë·ªãnh ch√≠nh.
    """
    if words_df.empty:
        return []
    y_min, y_max = words_df["y"].min(), words_df["y"].max()
    header = words_df[words_df["y"] <= (y_min + 0.30 * (y_max - y_min))].copy()
    if header.empty:
        return []
    xs = []
    for _, r in header.iterrows():
        t = (r["text"] or "").strip()
        tl = t.lower()
        if NUM_ANY.fullmatch(t):
            continue
        if len(t) < 3:
            continue
        if any(k in tl for k in ["ƒë∆°n v·ªã", "don vi", "vnd", "c·ªôt", "cot", "m·ª•c", "muc"]):
            continue
        xs.append(int(r["x"]))
    xs.sort()
    centers = []
    for x in xs:
        if not centers or abs(x - centers[-1]) > 55:
            centers.append(x)
        else:
            centers[-1] = int((centers[-1] + x) / 2)
    return centers


def _max_big_numbers_per_line(words_df: pd.DataFrame) -> int:
    """
    ƒê·∫øm s·ªë l∆∞·ª£ng gi√° tr·ªã s·ªë "l·ªõn" (>=5 ch·ªØ s·ªë) t·ªëi ƒëa tr√™n 1 d√≤ng.
    D√πng ƒë·ªÉ ph√¢n bi·ªát trang text vs trang b·∫£ng.
    """
    if words_df.empty:
        return 0
    lines = _group_lines(words_df, y_tol=8)
    mx = 0
    for ln in lines:
        texts = ln["text"].tolist()
        xs    = ln["x"].tolist()
        merged = _merge_numeric_runs(texts, xs, gap_px=110)
        cnt = 0
        for _x, raw in merged:
            if len(re.sub(r"[^\d]", "", str(raw))) >= 5:
                cnt += 1
        mx = max(mx, cnt)
    return mx


def _extract_header_text_near(words_df: pd.DataFrame, x: int, tol: int = 200) -> str:
    if words_df.empty:
        return ""
    y_min, y_max = words_df["y"].min(), words_df["y"].max()
    y_cut = y_min + 0.20 * (y_max - y_min)
    header = words_df[words_df["y"] <= y_cut]
    col_words = header[(header["x"] >= x - tol) & (header["x"] <= x + tol)]
    return " ".join(col_words["text"].tolist()).strip().lower()


def _normalize_context_from_header(txt: str) -> Tuple[str, str]:
    """
    Chuy·ªÉn header text -> context_key + context_label.
    B·ªô nh·∫≠n di·ªán ƒë∆°n gi·∫£n, ƒë·ªß d√πng cho notes.
    """
    t = (txt or "").lower()

    m = re.search(r'qu[√Ωy]\s*(\d+)\s*[\/\-]?\s*(20\d{2})', t)
    if m:
        return (f"q{m.group(1)}_{m.group(2)}", f"Qu√Ω {m.group(1)}/{m.group(2)}")

    if any(k in t for k in ["k·ª≥ k·∫ø to√°n", "ky ke toan", "l≈©y k·∫ø", "luy ke", "6 th√°ng", "6 thang"]):
        y = re.search(r'(20\d{2})', t)
        return (f"ytd_{y.group(1) if y else 'unk'}", "YTD")

    dm = DATE_RE.findall(t)
    if dm:
        y = re.search(r'(20\d{2})', dm[-1])
        return (f"asof_{y.group(1) if y else 'unk'}", f"As of {dm[-1]}")

    if "ƒë·∫ßu k·ª≥" in t or "opening" in t:
        return ("opening", "ƒê·∫ßu k·ª≥")
    if "cu·ªëi k·ª≥" in t or "closing" in t:
        return ("closing", "Cu·ªëi k·ª≥")

    # fallback: gi·ªØ nguy√™n text ƒë·ªÉ parser suy lu·∫≠n ti·∫øp
    return ("col", txt or "")


def _cluster_numeric_columns(words_df: pd.DataFrame, max_k: int = 10) -> List[int]:
    """
    Gom c·ª•m c√°c to·∫° ƒë·ªô x c·ªßa s·ªë li·ªáu ‚Üí suy ra v·ªã tr√≠ c·ªôt.

    - D√πng _max_big_numbers_per_line ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng s·ªë c·ªôt t·ªëi thi·ªÉu (min_k).
    - S·ªë d√πng ƒë·ªÉ detect c·ªôt: chu·ªói c√≥ >=4 ch·ªØ s·ªë (b·ªõt b·ªè s√≥t c·ªôt c√≥ s·ªë nh·ªè).
    - Kh√¥ng lo·∫°i qu√° m·∫°nh c√°c center √≠t ƒëi·ªÉm ƒë·ªÉ tr√°nh m·∫•t c·ªôt hi·∫øm.
    """
    if words_df.empty:
        return []

    xs_points: List[int] = []
    for ln in _group_lines(words_df, y_tol=8):
        texts = ln["text"].tolist()
        xs    = ln["x"].tolist()
        merged = _merge_numeric_runs(texts, xs, gap_px=110)
        for x, raw in merged:
            digits = re.sub(r"[^\d]", "", str(raw))
            if len(digits) >= 4:
                xs_points.append(int(x))

    if not xs_points:
        return []

    X = np.array(xs_points).reshape(-1, 1)

    max_big = _max_big_numbers_per_line(words_df)
    start_k = max(2, min(max_big, max_k))
    end_k   = min(max_k, len(X))

    best_centers, best_score = None, -1.0
    for k in range(start_k, end_k + 1):
        try:
            km = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X)
            if len(set(km.labels_)) < 2:
                continue
            sc = silhouette_score(X, km.labels_)

            prefer = (best_centers is not None and k > len(best_centers) and sc >= best_score - 0.02)
            if sc > best_score or prefer:
                best_score = sc
                best_centers = [int(c[0]) for c in km.cluster_centers_]
        except Exception:
            continue

    centers_num = sorted(best_centers) if best_centers else []

    # KH√îNG lo·∫°i m·∫°nh c√°c center "y·∫øu" n·ªØa, ch·ªâ g·ªôp v·ªõi header
    centers_hdr = _header_column_centers(words_df)

    all_c = sorted(centers_num + centers_hdr)
    merged: List[int] = []
    for x in all_c:
        if not merged or abs(x - merged[-1]) > 55:
            merged.append(int(x))
        else:
            merged[-1] = int((merged[-1] + x) // 2)

    if len(merged) < 2:
        return merged
    return merged[:max_k]


def _infer_multi_columns(words_df: pd.DataFrame) -> List[Dict]:
    """
    Tr·∫£ list [{x, context_key, context_label}] t·ª´ tr√°i sang ph·∫£i.
    Kh√¥ng c√≥ c·ªôt n√†o ‚Üí tr·∫£ [].
    """
    centers = _cluster_numeric_columns(words_df, max_k=8)
    if not centers or len(centers) < 2:
        return []

    cols: List[Dict] = []
    used = set()
    for idx, x in enumerate(sorted(centers), 1):
        h = _extract_header_text_near(words_df, x)
        key, lbl = _normalize_context_from_header(h)
        if key == "col":
            key = f"col{idx}"
        if key in used:
            key = f"{key}_{idx}"
        used.add(key)
        cols.append({"x": int(x), "context_key": key, "context_label": lbl})
    return cols


def _parse_line_with_multi_columns(line_df: pd.DataFrame, cols: List[Dict], tol=100):
    """
    Parse 1 d√≤ng notes OCR nhi·ªÅu c·ªôt:
      - Label = t·∫•t c·∫£ token text (kh√¥ng ph·∫£i s·ªë), s·∫Øp x·∫øp theo x ƒë·ªÉ tr√°nh ƒë·∫£o t·ª´.
      - S·ªë li·ªáu = c√°c s·ªë sau khi merge, lo·∫°i s·ªë qu√° b√™n tr√°i v√πng label.
    """
    if line_df.empty or not cols:
        return "", {}

    texts = line_df["text"].tolist()
    xs    = line_df["x"].tolist()
    ws    = line_df.get("w", pd.Series([20] * len(xs))).tolist()

    left_most_col = min(c["x"] for c in cols)
    median_w = int(np.median([w for w in ws if w and w > 0])) if ws else 24
    margin   = max(30, int(1.5 * median_w))
    label_threshold_x = left_most_col - margin

    # 1) Gom s·ªë & lo·∫°i s·ªë ·ªü v√πng label
    merged = _merge_numeric_runs(texts, xs, gap_px=62)
    nums: List[Tuple[int, str]] = []
    for x, raw in merged:
        if x < label_threshold_x:
            continue
        digits = re.sub(r"[^\d]", "", raw or "")
        if len(digits) >= 3:
            nums.append((int(x), raw))

    if not nums:
        return "", {}

    # 2) Gh√©p label t·ª´ t·∫•t c·∫£ token text (kh√¥ng ph·∫£i s·ªë), s·∫Øp theo x
    label_tokens: List[Tuple[str, int]] = []
    for t, x in zip(texts, xs):
        t_clean = (t or "").strip()
        if not t_clean:
            continue
        if NUMISH.match(t_clean):
            continue
        # b·ªè 'VND', 'VNƒê' ƒë·ª©ng ·ªü cu·ªëi
        if t_clean.lower() in {"vnd", "vnƒë"}:
            continue
        label_tokens.append((t_clean, x))

    label_tokens_sorted = sorted(label_tokens, key=lambda z: z[1])
    label = " ".join(t for t, _x in label_tokens_sorted).strip()
    label = _normalize_label_text(label)

    # 3) G√°n s·ªë v√†o t·ª´ng c·ªôt (gi·ªëng b√™n ocr_processor)
    cols_sorted = sorted([(c["x"], c["context_key"]) for c in cols], key=lambda z: z[0])
    nums_sorted = sorted(nums, key=lambda z: z[0])

    def assign_once(limit: int):
        assigned, used_num = {}, set()
        pairs = []
        for ci, (cx, ck) in enumerate(cols_sorted):
            for ni, (nx, sv) in enumerate(nums_sorted):
                d = abs(nx - cx)
                if d <= limit:
                    pairs.append((d, ci, ni))
        for d, ci, ni in sorted(pairs, key=lambda z: z[0]):
            if ci in assigned or ni in used_num:
                continue
            assigned[ci] = ni
            used_num.add(ni)
        return assigned

    assigned = assign_once(tol)
    if len(assigned) < len(cols_sorted):
        assigned = assign_once(int(2.2 * tol))

    values: Dict[str, str] = {}
    if len(assigned) == 0 and 1 <= len(nums_sorted) <= len(cols_sorted) + 1:
        for (cx, ck), (_nx, sv) in zip(cols_sorted, nums_sorted):
            values[ck] = sv
    else:
        for ci, (cx, ck) in enumerate(cols_sorted):
            if ci in assigned:
                ni = assigned[ci]
                values[ck] = nums_sorted[ni][1]

    return label, values



def _page_is_tabular(words: pd.DataFrame) -> bool:
    """
    Heuristic m·∫°nh ƒë·ªÉ ph√¢n bi·ªát trang thuy·∫øt minh d·∫°ng text vs trang c√≥ b·∫£ng s·ªë li·ªáu:
      - Ph·∫£i c√≥ √≠t nh·∫•t 2 c·ªôt s·ªë l·ªõn (>=5 ch·ªØ s·ªë)
      - M·ªói c·ªôt c√≥ >=3 ƒëi·ªÉm s·ªë.
    """
    if words.empty:
        return False
    if _max_big_numbers_per_line(words) < 2:
        # m·ªói d√≤ng t·ªëi ƒëa <2 s·ªë l·ªõn ‚Üí nhi·ªÅu kh·∫£ nƒÉng ch·ªâ l√† text, kh√¥ng ph·∫£i b·∫£ng
        return False
    centers = _cluster_numeric_columns(words, max_k=8)
    return len(centers) >= 2


def _extract_ocr_table(pdf_path: Path, pageno1: int) -> Optional[pd.DataFrame]:
    """
    D√≤ b·∫£ng b·∫±ng OCR cho 1 trang:
      - Log to√†n b·ªô d√≤ng OCR ƒë·ªçc ƒë∆∞·ª£c (DEBUG_OCR_LINES)
      - C·∫Øt header/footer
      - Suy ƒëo√°n nhi·ªÅu c·ªôt s·ªë t·ª´ s·ªë l·ªõn (>=5 ch·ªØ s·ªë)
      - Parse t·ª´ng d√≤ng -> DataFrame(desc, col_1_label, col_2_label,...)

    C√°c trang thuy·∫øt minh thu·∫ßn text (ch√≠nh s√°ch k·∫ø to√°n...) s·∫Ω b·ªã lo·∫°i
    v√¨ kh√¥ng ƒë·∫°t ti√™u ch√≠ ‚Äútrang b·∫£ng‚Äù.
    """
    img = _ocr_image(pdf_path, pageno1)
    words = _ocr_words(img)
    if words.empty:
        return None

    # Log raw OCR lines tr∆∞·ªõc m·ªçi filter
    _debug_dump_ocr_lines(words, pageno1)

    # C·∫Øt ph·∫ßn gi·ªØa trang (b·ªè header/footer m·∫°nh ch·ªØ "nƒÉm yyyy")
    y_min, y_max = words["y"].min(), words["y"].max()
    height = max(1, y_max - y_min)
    work = words[(words["y"] >= y_min + 0.08 * height) &
                 (words["y"] <= y_max - 0.12 * height)].copy()
    if work.empty:
        work = words

    # N·∫øu trang kh√¥ng c√≥ c·∫•u tr√∫c b·∫£ng r√µ r√†ng ‚Üí b·ªè
    if not _page_is_tabular(work):
        print(f"üîé [OCR] page {pageno1}: looks like narrative text, skip as table.")
        return None

    cols = _infer_multi_columns(work)

    # Fallback: n·∫øu v·∫´n kh√¥ng suy ra ƒë∆∞·ª£c multi-column th√¨ th√¥i, kh√¥ng coi l√† b·∫£ng OCR
    if not cols:
        print(f"‚ö†Ô∏è [OCR] page {pageno1}: cannot infer numeric columns, skip.")
        return None

    # T√™n c·ªôt hi·ªÉn th·ªã (header) ‚Äì d√πng context_label n·∫øu c√≥, ng∆∞·ª£c l·∫°i fallback "col1.."
    header_names: List[str] = ["desc"]
    for idx, c in enumerate(cols, 1):
        label = c["context_label"] or c["context_key"] or f"col{idx}"
        label = label.strip() or f"col{idx}"
        header_names.append(label)

    lines = _group_lines(work, y_tol=8)

    recs: List[dict] = []
    pending_label = ""

    for ln in lines:
        label, vals = _parse_line_with_multi_columns(ln, cols)
        label = _normalize_label_text(label)

        # D√≤ng kh√¥ng c√≥ s·ªë ‚Üí t√≠ch lu·ªπ label (d√≤ng ti·∫øp theo c√≥ s·ªë s·∫Ω gh√©p v√†o)
        if not vals:
            if label and not any(ch.isdigit() for ch in label):
                if pending_label:
                    pending_label = f"{pending_label} {label}"
                else:
                    pending_label = label
            continue

        # D√≤ng c√≥ s·ªë ‚Üí gh√©p label t√≠ch lu·ªπ
        if pending_label:
            if label:
                label = f"{pending_label} {label}"
            else:
                label = pending_label
            pending_label = ""

        if not label:
            label = "(no_label)"

        row: Dict[str, Optional[str]] = {header_names[0]: label}
        for j, c in enumerate(cols, 1):
            ctx_key = c["context_key"]
            col_name = header_names[j]
            row[col_name] = vals.get(ctx_key)
        recs.append(row)

    if not recs:
        return None

    df = pd.DataFrame(recs)
    # ƒë·∫£m b·∫£o th·ª© t·ª± c·ªôt ·ªïn ƒë·ªãnh: desc, col1, col2, ...
    df = df.reindex(columns=header_names)
    df = df.drop_duplicates()
    return df

def _process_one_page_tables(pdf_path: Path, p: int) -> List[Tuple[pd.DataFrame, int, str]]:
    """
    X·ª≠ l√Ω to√†n b·ªô b·∫£ng tr√™n 1 trang:
      - pdfplumber (vector)
      - camelot (stream)
      - OCR multi-column (cho b·∫£ng scan)
    Tr·∫£ v·ªÅ list (df, page, mode)
    """
    page_out: List[Tuple[pd.DataFrame, int, str]] = []

    # pdfplumber vector
    for df in _extract_plumber_tables(pdf_path, p):
        page_out.append((df, p, "vector"))

    # camelot stream
    for df in _extract_camelot_tables(pdf_path, p):
        page_out.append((df, p, "camelot"))

    # OCR multi-column
    mdf = _extract_ocr_table(pdf_path, p)
    if mdf is not None and not mdf.empty:
        page_out.append((mdf, p, "ocr"))

    return page_out

def harvest_tables(pdf_path: Path, a: int, b: int) -> List[Tuple[pd.DataFrame, int, str]]:
    """
    Qu√©t [a,b] ‚Üí [(df, page, mode)] v·ªõi mode ‚àà {"vector","camelot","ocr"}.

    T·ªëi ∆∞u:
      - M·ªói trang ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªôc l·∫≠p trong thread ri√™ng ‚Üí t·∫≠n d·ª•ng ƒëa core.
      - M·ªói thread v·∫´n l√†m ƒë·ªß 3 b∆∞·ªõc (plumber / camelot / OCR), kh√¥ng b·ªè d·ªØ li·ªáu.
    """
    out: List[Tuple[pd.DataFrame, int, str]] = []
    if a > b:
        return out

    pages = list(range(a, b + 1))
    # S·ªë worker h·ª£p l√Ω: min(s·ªë trang, s·ªë CPU logic - 1)
    max_workers = min(len(pages), max(1, multiprocessing.cpu_count() - 1))
    print(f"üßµ harvest_tables: pages={pages}, max_workers={max_workers}")

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(_process_one_page_tables, pdf_path, p): p for p in pages}
        for f in as_completed(futures):
            p = futures[f]
            try:
                res = f.result()
                if res:
                    out.extend(res)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω page {p} trong harvest_tables: {e}")

    # Gi·ªØ nguy√™n th·ª© t·ª± sort theo page (v√† kh√¥ng quan tr·ªçng mode)
    out.sort(key=lambda t: t[1])
    return out
