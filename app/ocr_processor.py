from pathlib import Path
from typing import List, Optional, Tuple, Dict
import os, re, traceback, sys
import io
import pytesseract
from pdf2image import convert_from_path
from PIL import Image
import pandas as pd
import numpy as np
import multiprocessing
from app.etl.normalize import normalize_with_unit
from concurrent.futures import ThreadPoolExecutor, as_completed 
from app.etl.table_finder import detect_unit, locate_statement_pages
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


_EASYOCR_READER = None
# ====== ti·ªán √≠ch detect ƒëang ch·∫°y exe hay ch·∫°y python b√¨nh th∆∞·ªùng ======
def _running_frozen() -> bool:
    return getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS")

def _get_base_dir() -> Path:
    if _running_frozen():
        return Path(sys._MEIPASS)
    # c√≤n ƒëang dev th√¨ d√πng th∆∞ m·ª•c file hi·ªán t·∫°i
    return Path(__file__).resolve().parent

BASE_DIR = _get_base_dir()

# ====== C·∫§U H√åNH TESSERACT & POPPLER ======
# ∆∞u ti√™n th∆∞ m·ª•c ƒëi k√®m exe: <base>/tesseract, <base>/tessdata, <base>/poppler_bin
TESSERACT_PATH = BASE_DIR / "tesseract" / "tesseract.exe"
POPPLER_PATH   = BASE_DIR / "poppler_bin"

if not TESSERACT_PATH.exists():
    TESSERACT_PATH = Path(r"C:\Program Files\Tesseract-OCR\tesseract.exe")

tessdata_dir = BASE_DIR / "tessdata"
if not tessdata_dir.exists():
    tessdata_dir = Path(r"C:\Program Files\Tesseract-OCR\tessdata")
os.environ["TESSDATA_PREFIX"] = str(tessdata_dir)

if TESSERACT_PATH.exists():
    pytesseract.pytesseract.tesseract_cmd = str(TESSERACT_PATH)
    print(f"‚úÖ ƒê√£ c·∫•u h√¨nh Tesseract: {TESSERACT_PATH}")
else:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tesseract.exe ‚Äì OCR Tesseract s·∫Ω l·ªói n·∫øu d√πng.")

_vie_path = tessdata_dir / "vie.traineddata"
if not _vie_path.exists():
    print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y {_vie_path}. H√£y c√†i/ch√©p 'vie.traineddata' v√†o th∆∞ m·ª•c tessdata.")
else:
    print(f"‚úÖ ƒê√£ tr·ªè TESSDATA_PREFIX ƒë·∫øn: {tessdata_dir}")

def pdf_to_images(pdf_path: Path,
                  dpi: int = 280,
                  first_page: Optional[int] = None,
                  last_page: Optional[int] = None) -> List[Image.Image]:
    """
    Render PDF -> list ·∫£nh (PIL Image) v·ªõi poppler n·∫øu c√≥.
    N·∫øu first_page/last_page ƒë∆∞·ª£c set th√¨ ch·ªâ render ƒëo·∫°n [first_page, last_page].
    """
    kwargs = {"dpi": dpi}
    if first_page is not None and last_page is not None:
        kwargs["first_page"] = first_page
        kwargs["last_page"] = last_page

    if POPPLER_PATH.exists():
        kwargs["poppler_path"] = str(POPPLER_PATH)

    return convert_from_path(str(pdf_path), **kwargs)

NUM_RE = re.compile(r"\(?[-+]?\s*(?:\d{1,3}(?:[.\s]\d{3})+|\d+)(?:,\d+|\.\d+)?\)?")
NUMISH_RE = re.compile(r'^[\d\.\,\(\)\-\s]+$')

CLEAN_NUM_CHARS_RE = re.compile(r"[^\d\-\+,\.]")
def _clean_numeric_str(raw: str) -> str:
    """
    Chu·∫©n ho√° chu·ªói s·ªë OCR tr∆∞·ªõc khi parse:
      - B·ªè ngo·∫∑c tr√≤n bao quanh: (123) -> 123  (coi nh∆∞ s·ªë d∆∞∆°ng)
      - B·ªè kho·∫£ng tr·∫Øng th·ª´a, k√Ω t·ª± kh√¥ng ph·∫£i 0-9 . , + -
    """
    if raw is None:
        return ""
    s = str(raw).strip()
    if not s:
        return ""
    if s.startswith("(") and s.endswith(")"):
        s = s[1:-1]
    s = CLEAN_NUM_CHARS_RE.sub("", s)
    return s

# M·ªôt s·ªë pattern label b·ªã OCR ƒë·∫£o t·ª´ ƒë·∫∑c tr∆∞ng c·ªßa VAS
LABEL_FIX_PATTERNS = {
    "ti·ªÅn kh√¥ng k·ª≥ h·∫°n h√†ng g·ª≠i ng√¢n": "Ti·ªÅn g·ª≠i ng√¢n h√†ng kh√¥ng k·ª≥ h·∫°n",
    "chuy√™n ti·ªÅn ƒëang": "Ti·ªÅn ƒëang chuy·ªÉn",
    "ti·ªÅn ƒë∆∞∆°ng kho·∫£n c√°c t∆∞∆°ng": "C√°c kho·∫£n t∆∞∆°ng ƒë∆∞∆°ng ti·ªÅn",
}

def _normalize_label_text(label: str) -> str:
    """
    S·ª≠a c√°c label b·ªã ƒë·∫£o t·ª´/c·ª•m t·ª´ r·∫•t hay g·∫∑p trong thuy·∫øt minh.
    C√°c case kh√°c ƒë·ªÉ nguy√™n.
    """
    if not label:
        return label
    low = re.sub(r"\s+", " ", label.lower()).strip().strip(":")
    for bad, good in LABEL_FIX_PATTERNS.items():
        if bad in low:
            return good
    return label.strip()

def _merge_numeric_runs(texts, xs, gap_px: int = 60):
    """
    Gh√©p c√°c token s·ªë ƒë·ª©ng li·ªÅn nhau th√†nh 1 s·ªë ƒë·∫ßy ƒë·ªß.
    - Cho ph√©p c√≥ ngo·∫∑c ( ) bao quanh, nh∆∞ng khi tr·∫£ ra s·∫Ω b·ªè ngo·∫∑c
    - Lo·∫°i b·ªè k√Ω t·ª± l·∫°, gi·ªØ l·∫°i 0-9 . , + -
    Tr·∫£ v·ªÅ list (x_left, cleaned_for_parse, raw_concat)
    """
    items = sorted([(x, t) for t, x in zip(texts, xs)], key=lambda z: z[0])
    out = []
    buf_raw, buf_x = "", None
    prev_x = None

    def _flush():
        nonlocal buf_raw, buf_x
        if not buf_raw:
            return
        cleaned = _clean_numeric_str(buf_raw)
        if not cleaned or not re.search(r"\d", cleaned):
            buf_raw, buf_x = "", None
            return
        out.append((buf_x, cleaned, buf_raw.strip()))
        buf_raw, buf_x = "", None

    for x, t in items:
        t = (t or "").strip()
        if not t:
            continue
        # token kh√¥ng ph·∫£i d·∫°ng s·ªë ‚Üí ƒë√≥ng c·ª•m hi·ªán t·∫°i
        if not NUMISH_RE.match(t):
            _flush()
            prev_x = x
            continue

        if buf_raw == "":
            buf_raw = t
            buf_x = x
        else:
            near = prev_x is not None and (x - prev_x) <= gap_px
            thousand_glue = buf_raw.rstrip().endswith((".", ",")) or re.search(r"[\.\,]\s*$", buf_raw)
            only_2_3_digits = len(re.sub(r"\D", "", t)) in (2, 3)
            if near or (thousand_glue and only_2_3_digits):
                buf_raw += t
            else:
                _flush()
                buf_raw = t
                buf_x = x
        prev_x = x

    _flush()

    # b·ªè c·ª•m qu√° nh·ªè (√≠t h∆°n 3 ch·ªØ s·ªë ‚Üí th∆∞·ªùng l√† nƒÉm, s·ªë trang‚Ä¶)
    filtered = []
    for x, cleaned, raw in out:
        digits = re.sub(r"\D", "", cleaned)
        if len(digits) >= 3:
            filtered.append((x, cleaned, raw))
    return filtered


def _ocr_words_tesseract(image) -> pd.DataFrame:
    cfg = r'--oem 3 --psm 6 -l vie+eng -c preserve_interword_spaces=1'
    d = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT, config=cfg)
    rows = []
    for i, txt in enumerate(d["text"]):
        try:
            conf = float(d["conf"][i])
        except Exception:
            conf = -1.0
        txt_clean = (txt or "").strip()
        if not txt_clean or len(txt_clean) < 1:
            continue
        if conf < 40:
            continue
        rows.append({
            "text": txt_clean,
            "x": d["left"][i],
            "y": d["top"][i],
            "w": d["width"][i],
            "h": d["height"][i],
            "conf": conf
        })
    return pd.DataFrame(rows)


def ocr_words(image, engine: str = "tesseract") -> pd.DataFrame:
    return _ocr_words_tesseract(image)

def _group_lines(df: pd.DataFrame, y_tol=8) -> List[pd.DataFrame]:
    """Gom d√≤ng ch√≠nh x√°c h∆°n"""
    if df.empty: return []
    df = df.sort_values(["y","x"]).reset_index(drop=True)
    lines, cur = [], [df.iloc[0]]
    for i in range(1, len(df)):
        prev = cur[-1]; row = df.iloc[i]
        if abs(row["y"] - prev["y"]) <= y_tol:
            cur.append(row)
        else:
            lines.append(pd.DataFrame(cur))
            cur = [row]
    if cur: lines.append(pd.DataFrame(cur))
    return lines

HEADER_CUR = re.compile(r"(nƒÉm\s*nay|s·ªë\s*cu·ªëi\s*k·ª≥|k·ª≥\s*n√†y)", re.I|re.U)
HEADER_PRI = re.compile(r"(nƒÉm\s*tr∆∞·ªõc|s·ªë\s*ƒë·∫ßu\s*nƒÉm|k·ª≥\s*tr∆∞·ªõc)", re.I|re.U)

def _peak_columns(nums_x: List[int]) -> List[int]:
    """Tr·∫£ v·ªÅ ~t·ªça ƒë·ªô trung t√¢m 2 c·ªôt s·ªë (x) t·ª´ danh s√°ch x c·ªßa c√°c tokens s·ªë trong trang."""
    if len(nums_x) < 6:
        return sorted(set(nums_x))[-2:] if len(set(nums_x)) >= 2 else sorted(set(nums_x))
    xs = np.array(sorted(nums_x))
    # K-means K=2 theo 1D x
    c1, c2 = xs.min(), xs.max()
    for _ in range(12):
        g1 = xs[np.abs(xs - c1) <= np.abs(xs - c2)]
        g2 = xs[np.abs(xs - c2) <  np.abs(xs - c1)]
        nc1 = float(g1.mean()) if len(g1) else c1
        nc2 = float(g2.mean()) if len(g2) else c2
        if abs(nc1-c1)+abs(nc2-c2) < 1.0: break
        c1, c2 = nc1, nc2
    return sorted([int(c1), int(c2)])

def _infer_col_roles(words_df: pd.DataFrame) -> Dict[str,int]:
    """X√°c ƒë·ªãnh c·ªôt v·ªõi logic c·∫£i ti·∫øn cho b·∫£ng kh√¥ng khung"""
    if words_df.empty: 
        return {"current_x": None, "prior_x": None}
    
    # 1) T√¨m t·∫•t c·∫£ c√°c token s·ªë v√† l·ªçc nhi·ªÖu
    num_df = words_df[words_df["text"].str.match(NUM_RE, na=False)].copy()
    if num_df.empty:
        return {"current_x": None, "prior_x": None}
    
    # L·ªçc nhi·ªÖu: lo·∫°i b·ªè s·ªë qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá
    def is_valid_number(txt):
        clean = re.sub(r'[^\d]', '', txt)
        return len(clean) >= 6  # Ch·ªâ l·∫•y s·ªë c√≥ √≠t nh·∫•t 6 ch·ªØ s·ªë
    
    num_df = num_df[num_df["text"].apply(is_valid_number)]
    if num_df.empty:
        return {"current_x": None, "prior_x": None}
    
    # 2) Gom c·ª•m theo c·ªôt
    xs = np.array(num_df["x"].tolist())
    if len(xs) < 4:
        unique_xs = sorted(set(xs))
        if len(unique_xs) >= 2:
            left_x, right_x = unique_xs[0], unique_xs[-1]
        else:
            return {"current_x": None, "prior_x": None}
    else:
        try:
            # S·ª≠ d·ª•ng K-means v·ªõi 2 c·ª•m
            kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)
            kmeans.fit(xs.reshape(-1, 1))
            centers = sorted(kmeans.cluster_centers_.flatten())
            left_x, right_x = int(centers[0]), int(centers[1])
        except:
            # Fallback: l·∫•y min v√† max
            left_x, right_x = int(xs.min()), int(xs.max())
    
    # 3) X√°c ƒë·ªãnh current/prior d·ª±a tr√™n header
    y_cut = words_df["y"].min() + 0.12 * (words_df["y"].max() - words_df["y"].min())
    header = words_df[words_df["y"] <= y_cut]
    
    def get_col_text(col_x, tol=120):
        col_words = header[
            (header["x"] >= col_x - tol) & 
            (header["x"] <= col_x + tol)
        ]
        return " ".join(col_words["text"].tolist()).lower()
    
    left_text = get_col_text(left_x)
    right_text = get_col_text(right_x)
    
    print(f"   Header left: '{left_text}'")
    print(f"   Header right: '{right_text}'")
    
    # 4) Quy·∫øt ƒë·ªãnh mapping d·ª±a tr√™n ng√†y th√°ng r√µ r√†ng
    left_has_cur = any(date_str in left_text for date_str in ["30/06/2025", "30/6/2025", "3062025"])
    right_has_cur = any(date_str in right_text for date_str in ["30/06/2025", "30/6/2025", "3062025"])
    left_has_pri = any(date_str in left_text for date_str in ["01/01/2025", "1/1/2025", "01012025"])
    right_has_pri = any(date_str in right_text for date_str in ["01/01/2025", "1/1/2025", "01012025"])
    
    # ∆Øu ti√™n c√°c pattern r√µ r√†ng
    if left_has_cur and right_has_pri:
        return {"current_x": left_x, "prior_x": right_x}
    elif right_has_cur and left_has_pri:
        return {"current_x": right_x, "prior_x": left_x}
    elif "30/06/2025" in left_text or "30/6/2025" in left_text:
        return {"current_x": left_x, "prior_x": right_x}
    elif "30/06/2025" in right_text or "30/6/2025" in right_text:
        return {"current_x": right_x, "prior_x": left_x}
    else:
        # Fallback: d·ª±a v√†o v·ªã tr√≠ (th∆∞·ªùng current b√™n tr√°i trong VAS)
        return {"current_x": left_x, "prior_x": right_x}

def _parse_line_with_columns(line_df: pd.DataFrame, current_x: int, prior_x: int, tol=100):
    texts, xs = line_df["text"].tolist(), line_df["x"].tolist()

    # GH√âP S·ªê b·ªã t√°ch
    merged = _merge_numeric_runs(texts, xs, gap_px=60)
    nums = []
    for x, clean_text, raw in merged:
        if clean_text and len(re.sub(r'[^\d]', '', clean_text)) >= 4:
            nums.append((x, clean_text, raw))
    if not nums:
        return "", None, None

    # label m·ªÅm h∆°n
    first_num_x = min(x for x, _n, _r in nums)
    threshold_x = min(first_num_x, current_x, prior_x) - 20
    label_tokens = [t for t, x in zip(texts, xs) if x < threshold_x and not NUMISH_RE.match(t)]
    if not label_tokens:
        label_tokens = [t for t, x in zip(texts, xs) if x < threshold_x]
    label = " ".join(label_tokens).strip()

    def find_best_match(target_x, numbers, tolerance=tol):
        if not numbers:
            return None
        distances = [(abs(x - target_x), num) for x, num, _r in numbers]
        distances.sort(key=lambda z: z[0])
        if distances and distances[0][0] <= tolerance:
            return distances[0][1]
        return None

    cur_raw = find_best_match(current_x, nums) or find_best_match(current_x, nums, tol * 1.6)
    pri_raw = find_best_match(prior_x, nums) or find_best_match(prior_x, nums, tol * 1.6)

    if label and (cur_raw or pri_raw):
        print(f"     Line: '{label[:50]}...' -> cur: {cur_raw}, prior: {pri_raw}")

    return label, pri_raw, cur_raw

def extract_tables_ocr(pdf_path: Path,
                       pages: Optional[List[int]] = None,
                       ocr_engine: str = "tesseract",
                       max_workers: Optional[int] = None) -> pd.DataFrame:
    """
    Tr·∫£ v·ªÅ DataFrame long-form:
        page, statement_hint, vas_code, src_label, context_key, context_label, amount, unit_hint, confidence

    T·ªëi ∆∞u:
      - locate_statement_pages ch·ªâ ch·∫°y 1 l·∫ßn ·ªü ƒë√¢y.
      - Ch·ªâ render ƒëo·∫°n trang c·∫ßn qu√©t (min..max).
      - X·ª≠ l√Ω OCR + parse b·∫£ng song song nhi·ªÅu thread.
      - H·ªó tr·ª£ ocr_engine='tesseract' (CPU) ho·∫∑c 'easyocr' (GPU n·∫øu c√†i).
    """
    # 1) X√°c ƒë·ªãnh v√πng trang c·∫ßn qu√©t
    page_ranges = locate_statement_pages(pdf_path, max_pages_to_scan=20)
    section_by_page: Dict[int, str] = {}
    for section, (start, end) in page_ranges.items():
        for p in range(start, end + 1):
            section_by_page[p] = section

    if pages:
        page_whitelist = set(pages) & set(section_by_page.keys())
    else:
        page_whitelist = set(section_by_page.keys())

    if not page_whitelist:
        print("‚ö†Ô∏è Kh√¥ng c√≥ trang n√†o ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ qu√©t b·∫£ng.")
        return pd.DataFrame([])

    page_whitelist = set(sorted(page_whitelist))
    min_page, max_page = min(page_whitelist), max(page_whitelist)

    print(f"üìñ T·ªïng s·ªë trang statement: {len(section_by_page)}")
    print(f"üîç Qu√©t c√°c trang: {sorted(page_whitelist)} (render t·ª´ {min_page} ƒë·∫øn {max_page})")

    # 2) Render ·∫£nh ch·ªâ ƒëo·∫°n [min_page, max_page]
    images_segment = pdf_to_images(pdf_path, dpi=280,
                                   first_page=min_page,
                                   last_page=max_page)
    # map page -> image
    page_to_img: Dict[int, Image.Image] = {}
    for idx, img in enumerate(images_segment, start=min_page):
        if idx in page_whitelist:
            page_to_img[idx] = img

    if not page_to_img:
        print("‚ö†Ô∏è Kh√¥ng render ƒë∆∞·ª£c ·∫£nh cho c√°c trang c·∫ßn qu√©t.")
        return pd.DataFrame([])

    # 3) Song song ho√° x·ª≠ l√Ω t·ª´ng trang
    all_rows: List[Dict] = []
    if max_workers is None:
        # s·ªë core h·ª£p l√Ω: min( s·ªë trang, s·ªë CPU logic )
        max_workers = min(len(page_to_img), max(1, multiprocessing.cpu_count() - 1))

    print(f"‚öôÔ∏è S·ª≠ d·ª•ng t·ªëi ƒëa {max_workers} worker ƒë·ªÉ OCR.")

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = []
        for pageno1, img in page_to_img.items():
            section = section_by_page.get(pageno1, "UNKNOWN")
            futures.append(
                ex.submit(_process_one_page_for_tables,
                          pageno1, img, section, ocr_engine, None)
            )

        for f in as_completed(futures):
            try:
                rows = f.result()
                if rows:
                    all_rows.extend(rows)
            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω 1 trang: {e}")
                traceback.print_exc()

    df = pd.DataFrame(all_rows)
    print(f"\nüìä T·ªïng c·ªông tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(df)} √¥ s·ªë li·ªáu ·ªü d·∫°ng long-form")
    return df


def _cluster_numeric_columns(words_df: pd.DataFrame, max_k: int = 5) -> List[int]:
    """
    Gom c·ª•m c√°c to·∫° ƒë·ªô x c·ªßa s·ªë li·ªáu ƒë·ªÉ suy ra s·ªë c·ªôt.
    Tr·∫£ v·ªÅ danh s√°ch center x (int) t·ª´ tr√°i -> ph·∫£i.
    """
    num_df = words_df[words_df["text"].str.match(NUM_RE, na=False)].copy()
    if num_df.empty:
        return []
    # l·ªçc s·ªë "ƒë·ªß d√†i" ƒë·ªÉ tr√°nh nhi·ªÖu
    def is_valid_number(txt):
        clean = re.sub(r'[^\d]', '', txt)
        return len(clean) >= 6
    xs = np.array([x for x, t in zip(num_df["x"].tolist(), num_df["text"].tolist()) if is_valid_number(t)])
    if len(xs) < 3:
        return sorted(list(set(map(int, xs))))
    xs = xs.reshape(-1, 1)

    best_k, best_score, best_centers = None, -1, None
    for k in range(2, max(2, min(max_k, len(xs)) + 1)):
        try:
            km = KMeans(n_clusters=k, random_state=0, n_init=10).fit(xs)
            if len(set(km.labels_)) < 2:  # silhouette c·∫ßn >=2 c·ª•m
                continue
            sc = silhouette_score(xs, km.labels_)
            if sc > best_score:
                best_score = sc
                best_k = k
                best_centers = sorted([int(c[0]) for c in km.cluster_centers_])
        except Exception:
            continue

    if best_centers:
        return best_centers
    # fallback
    uniq = sorted(list(set([int(v[0]) for v in xs])))
    return uniq[:max_k]

def _extract_header_text_near(words_df: pd.DataFrame, x: int, tol: int = 140) -> str:
    """L·∫•y text c·ªßa v√πng header g·∫ßn t√¢m c·ªôt x."""
    if words_df.empty:
        return ""
    y_min, y_max = words_df["y"].min(), words_df["y"].max()
    y_cut = y_min + 0.20 * (y_max - y_min)  
    header = words_df[words_df["y"] <= y_cut]
    col_words = header[(header["x"] >= x - tol) & (header["x"] <= x + tol)]
    return " ".join(col_words["text"].tolist()).strip().lower()

DATE_RE = re.compile(r'(\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4})')

def _normalize_context_from_header(txt: str) -> Tuple[str, str]:
    """
    Quy ƒë·ªïi header text -> context_key (machine-friendly) v√† context_label (gi·ªØ nguy√™n ƒë·ªÉ trace).
    ∆Øu ti√™n nh·∫≠n di·ªán:
        - Qu√Ω: 'q2/2025', 'qu√Ω 2/2025'
        - L≈©y k·∫ø/k·ª≥ k·∫ø to√°n: '01/01/2025 ƒë·∫øn 30/06/2025' => 'ytd_2025'
    N·∫øu m∆° h·ªì: tr·∫£ 'col1', 'col2', ...
    """
    t = (txt or "").lower()
    # quarterly
    m = re.search(r'qu[√Ωy]\s*(\d+)\s*[\/\-]?\s*(20\d{2})', t)
    if m:
        q, y = m.group(1), m.group(2)
        return (f"q{q}_{y}", f"Qu√Ω {q}/{y}")
    m = re.search(r'(q|quy)\s*(\d+)\s*[\/\-]?\s*(20\d{2})', t)
    if m:
        q, y = m.group(2), m.group(3)
        return (f"q{q}_{y}", f"Qu√Ω {q}/{y}")
    # ytd / k·ª≥ k·∫ø to√°n / l≈©y k·∫ø 6T
    if "k·ª≥ k·∫ø to√°n" in t or "ky ke toan" in t or "l≈©y k·∫ø" in t or "luy ke" in t or "6 th√°ng" in t or "6 thang" in t:
        # c·ªë g·∫Øng b·∫Øt nƒÉm: l·∫•y nƒÉm xu·∫•t hi·ªán trong chu·ªói
        y = None
        ym = re.search(r'(20\d{2})', t)
        if ym: y = ym.group(1)
        return (f"ytd_{y or 'unk'}", f"YTD {y or ''}".strip())
    # n·∫øu c√≥ ng√†y k·∫øt th√∫c ki·ªÉu dd/mm/yyyy -> d√πng nƒÉm l√†m context
    dm = DATE_RE.findall(t)
    if dm:
        # l·∫•y nƒÉm c·ªßa ng√†y cu·ªëi
        y = re.search(r'(20\d{2})', dm[-1])
        yv = y.group(1) if y else "unk"
        return (f"asof_{yv}", f"As of {dm[-1]}")
    # default
    return ("col", txt or "")

def _infer_multi_columns(words_df: pd.DataFrame) -> List[Dict]:
    """
    Tr·∫£ v·ªÅ danh s√°ch [{x, context_key, context_label}], t·ª´ tr√°i sang ph·∫£i.
    """
    centers = _cluster_numeric_columns(words_df, max_k=6)
    cols = []
    used_keys = set()
    for idx, x in enumerate(centers, 1):
        htext = _extract_header_text_near(words_df, x)
        key, label = _normalize_context_from_header(htext)
        # tr√°nh tr√πng key
        if key == "col":
            key = f"col{idx}"
        if key in used_keys:
            key = f"{key}_{idx}"
        used_keys.add(key)
        cols.append({"x": int(x), "context_key": key, "context_label": label})
    # l·ªçc nh·ªØng tr∆∞·ªùng h·ª£p nhi·ªÖu c√≥ <2 c·ªôt
    if len(cols) >= 2:
        return cols
    return []

def _parse_line_with_multi_columns(line_df: pd.DataFrame, cols: List[Dict], tol=100):
    texts = line_df["text"].tolist()
    xs = line_df["x"].tolist()
    # (1) GH√âP S·ªê b·ªã t√°ch: thay v√¨ duy·ªát token r·ªùi, d√πng merge
    merged_nums = _merge_numeric_runs(texts, xs, gap_px=62)
    nums = []
    for x, clean_text, raw in merged_nums:
        # y√™u c·∫ßu t·ªëi thi·ªÉu 5 ch·ªØ s·ªë ƒë·ªÉ tr√°nh nhi·ªÖu
        if clean_text and len(re.sub(r'[^\d]', '', clean_text)) >= 5:
            nums.append((x, clean_text, raw))
    if not nums:
        return "", {}

    # (2) L·∫§Y NH√ÉN: m·ªÅm h∆°n ‚Äì d·ª±a tr√™n v·ªã tr√≠ s·ªë ƒë·∫ßu ti√™n trong d√≤ng
    first_num_x = min(x for x, _n, _r in nums)
    left_most_col = min(c["x"] for c in cols)
    threshold_x = min(first_num_x, left_most_col) - 20

    # L·∫•y c√°c token label (kh√¥ng ph·∫£i s·ªë) n·∫±m tr∆∞·ªõc threshold, GH√âP THEO TH·ª® T·ª∞ X TƒÇNG D·∫¶N
    label_tokens = [(t, x) for t, x in zip(texts, xs) if x < threshold_x and not NUMISH_RE.match(t)]
    if not label_tokens:
        # fallback: l·∫•y m·ªçi token (k·ªÉ c·∫£ s·ªë VAS code ƒë·∫ßu d√≤ng) n·∫±m tr∆∞·ªõc threshold
        label_tokens = [(t, x) for t, x in zip(texts, xs) if x < threshold_x]
    # S·∫Øp x·∫øp theo x tƒÉng d·∫ßn
    label_tokens_sorted = sorted(label_tokens, key=lambda z: z[1])
    label = " ".join([t for t, x in label_tokens_sorted]).strip()

    def find_best_match(target_x, numbers, tolerance=tol):
        if not numbers:
            return None
        distances = sorted([(abs(x - target_x), num) for x, num, _raw in numbers], key=lambda z: z[0])
        if distances and distances[0][0] <= tolerance:
            return distances[0][1]
        return None

    values = {}
    for c in cols:
        raw = find_best_match(c["x"], nums) or find_best_match(c["x"], nums, tol * 1.6)
        if raw is not None:
            values[c["context_key"]] = raw

    return label, values

def _process_one_page_for_tables(pageno1: int,
                                 img,
                                 section: str,
                                 ocr_engine: str,
                                 cols_cache: Optional[List[Dict]] = None) -> List[Dict]:
    """
    X·ª≠ l√Ω 1 trang: OCR -> detect columns -> parse lines -> tr·∫£ list row long-form.
    D√πng cho ThreadPoolExecutor.
    """
    rows_out: List[Dict] = []

    print(f"\nüîç ƒêang x·ª≠ l√Ω trang {pageno1} (engine={ocr_engine})...")
    words = ocr_words(img, engine=ocr_engine)
    if words.empty:
        print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y text tr√™n trang {pageno1}")
        return rows_out

    words = _strip_page_headers(words)
    if words.empty:
        return rows_out

    unit_hint = detect_unit(" ".join(words["text"].tolist())) or "VND"

    # ==== multi-column detection ====
    cols = cols_cache or _infer_multi_columns(words)

    # Fallback c≈©: n·∫øu kh√¥ng ra nhi·ªÅu c·ªôt, th·ª≠ logic 2 c·ªôt
    if not cols:
        two = _infer_col_roles(words)
        if not two["current_x"] or not two["prior_x"]:
            print(f"   ‚ùå Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c v·ªã tr√≠ c·ªôt tr√™n trang {pageno1}")
            return rows_out
        cols = [
            {"x": int(two["prior_x"]),   "context_key": "prior_period",   "context_label": "Prior"},
            {"x": int(two["current_x"]), "context_key": "current_period", "context_label": "Current"},
        ]
    cols = sorted(cols, key=lambda c: c["x"])

    lines = _group_lines(words)
    print(f"   üìÑ T√¨m th·∫•y {len(lines)} d√≤ng | {len(cols)} c·ªôt")

    page_rows = 0
    for line in lines:
        label, values = _parse_line_with_multi_columns(line, cols)
        if not label:
            continue

        vas = None
        m = re.match(r"^\s*(\d{2,3}[A-Z]?)\s*[-.:]?\s*", label)
        if m:
            vas = m.group(1)
            label = re.sub(r"^\s*\d{2,3}[A-Z]?\s*[-.:]?\s*", "", label).strip()

        digit_ratio = sum(c.isdigit() for c in label) / max(1, len(label))
        if (digit_ratio > 0.6 and
            not any(k in label.lower() for k in ["t·ªïng", "c·ªông", "total", "100", "200", "300", "400"])):
            continue

        for c in cols:
            raw = values.get(c["context_key"])
            if raw is None:
                continue
            amt_vnd, _ = normalize_with_unit(raw, unit_hint)
            if amt_vnd is None:
                continue

            rows_out.append(dict(
                page=pageno1,
                statement_hint=section,
                vas_code=vas,
                src_label=label,
                context_key=c["context_key"],
                context_label=c["context_label"],
                amount=amt_vnd,
                unit_hint=unit_hint,
                confidence=0.9 if vas else 0.8
            ))
            page_rows += 1

    print(f"   ‚úÖ Tr√≠ch xu·∫•t ƒë∆∞·ª£c {page_rows} √¥ s·ªë li·ªáu t·ª´ trang {pageno1}")
    return rows_out

def _strip_page_headers(words_df: pd.DataFrame) -> pd.DataFrame:
    if words_df.empty:
        return words_df
    y_min, y_max = words_df["y"].min(), words_df["y"].max()
    height = y_max - y_min if y_max > y_min else 1
    # c·∫Øt header/footer
    top_cut = y_min + 0.08 * height
    bot_cut = y_max - 0.12 * height
    df = words_df[(words_df["y"] >= top_cut) & (words_df["y"] <= bot_cut)].copy()
    if df.empty:
        return words_df

    # lo·∫°i d√≤ng ‚ÄúnƒÉm 2025‚Äù, ‚ÄúH√† N·ªôi, ng√†y ... 2025‚Äù
    grouped = (
    df.sort_values(["y","x"])
      .groupby(pd.cut(df["y"], bins=max(1, int(height/20)), include_lowest=True), observed=False)
    )
    keep_rows = []
    for _bin, g in grouped:
        toks = g["text"].tolist()
        nums = [re.sub(r"\D","",t) for t in toks]
        years = [n for n in nums if len(n)==4 and 1900 <= int(n) <= 2100]
        # n·∫øu h∆°n 60% token l√† nƒÉm/s·ªë 4 ch·ªØ s·ªë ‚Üí b·ªè (r·∫•t hay g·∫∑p ·ªü header/footer)
        ratio_year = 0 if not toks else (len(years)/len(toks))
        if ratio_year > 0.6:
            continue
        keep_rows.append(g)
    if keep_rows:
        df = pd.concat(keep_rows, ignore_index=True)
    return df